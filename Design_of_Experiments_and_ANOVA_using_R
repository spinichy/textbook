##### part 1 -----------------------------------------------------------------------------------------------------------------------


##t-test

#Data Input
male <- c(327, 291, 323, 284, 305)
female <- c(308, 324, 353, 344, 341)

#H0: Variances are equal
var.test(male, female) #p-value = 0.9154

#Using Welch's t-test by default (var.equal = FALSE)
test_result <- t.test(male, female, var.equal = TRUE, alternative = "two.sided")
test_result #p-value = 0.04329: reject H0


##Anova - 2 variables
# 1. Data Input
male <- c(327, 291, 323, 284, 305)
female <- c(308, 324, 353, 344, 341)

# 2. Create Data Frame (Required for ANOVA)
scores <- c(male, female)
gender <- rep(c("Male", "Female"), each = 5)
df <- data.frame(gender, scores)

# 3. Perform ANOVA
anova_result <- aov(scores ~ gender, data = df)

# 4. View Results (Summary table)
summary(anova_result)

#verification
total_mean <- mean(df$scores)
group_mean <- aggregate(scores ~ gender, data = df, FUN = 'mean')
group_mean_Female <- group_mean %>% 
  filter(gender == 'Female') %>% 
  select(scores) %>% 
  as.numeric()
group_mean_Male <- group_mean %>% 
  filter(gender == 'Male') %>% 
  select(scores) %>% 
  as.numeric()

n_male <- length(male)
n_female <- length(female)

SST <- sum( (df$scores - total_mean)^2 )

SSR <- sum( n_female*(group_mean_Female - total_mean)^2 + 
              n_male*(group_mean_Male - total_mean)^2 )
SSE <- sum( (female - group_mean_Female)^2 ) + sum( (male - group_mean_Male)^2 )
MSR <- SSR/(2-1)
MSE <- SSE/(n_male - 1 + n_female - 1)

F_value <- MSR/MSE
pf(q = F_value, df1 = 1, df2 = 8, lower.tail = FALSE)


##Anova - 3 variables


# 1. Data Setup (Different sample sizes for each group)
# male (n=5), female (n=4), other (n=6)
male   <- c(327, 291, 323, 284, 305)
female <- c(308, 324, 353, 344)
other  <- c(315, 330, 310, 320, 325, 318)

# 2. Data Transformation
# Combine all scores
all_scores <- c(male, female, other)

# Create grouping labels corresponding to the number of data points in each group
group_labels <- c(rep("Male", length(male)), 
                  rep("Female", length(female)), 
                  rep("Other", length(other)))

# Create a data frame
df_unbalanced <- data.frame(
  group = factor(group_labels),
  score = all_scores
)

# 3. Check sample size for each group
table(df_unbalanced$group)

# 4. Perform One-way ANOVA  
# R's aov() handles unbalanced data using Type I Sum of Squares by default
anova_model <- aov(score ~ group, data = df_unbalanced)

# 5. Check ANOVA Summary Table
summary(anova_model)

# 6. Post-hoc Analysis (Tukey's HSD)
# TukeyHSD in R automatically adjusts for unequal sample sizes (Tukey-Kramer method)
tukey_result <- TukeyHSD(anova_model)
print(tukey_result)

# 7. Visualization
boxplot(score ~ group, data = df_unbalanced,
        main = "Unbalanced ANOVA: Score Comparison",
        xlab = "Group (Different Sample Sizes)",
        ylab = "Score",
        col = c("lightpink", "lightblue", "lightgreen"))


## exercise

##1.1 - 1


# 1. Load required libraries
library(sasLM)
library(ggplot2)

# 2. Data Preparation
# Load bondreturn dataset: Returns of bonds according to their ratings
sasLM::bondreturn

# 3. Exploratory Data Analysis (EDA)
# Check the internal structure and statistical summary
str(bondreturn)
summary(bondreturn)

# Check sample sizes for each group (Verifying Unbalanced Design)
table(bondreturn$Bond)

# 4. Create Boxplot comparing 'bank' vs 'corp'
ggplot(bondreturn, aes(x = Bond, y = Return, fill = Bond)) +
  geom_boxplot(alpha = 0.7, outlier.shape = NA) +
  geom_jitter(width = 0.1, color = "black", alpha = 0.5) +
  theme_minimal() +
  labs(
    title = "Comparison of Bond Returns: Bank vs. Corporation",
    subtitle = "Yield analysis between different bond sectors",
    x = "Bond Type",
    y = "Return Rate (%)"
  ) +
  # Use exact level names. If levels are capitalized, use "Bank" and "Corp"
  # Alternatively, use scale_fill_brewer for automatic mapping
  scale_fill_brewer(palette = "Set2") + 
  theme(legend.position = "none")


##1.1 - 2


# 1. Load library and data
library(sasLM)
sasLM::bondreturn

# 2. Check group levels and sample size
# Verify that there are exactly two groups: 'bank' and 'corp'
table(bondreturn$Bond)

# 3. Check for Homogeneity of Variance (Levene's Test)
# This determines whether to use a standard t-test or Welch's t-test
var.test(Return ~ Bond, data = bondreturn) #p-value = 0.7939

# 4. Perform Independent t-test
# t.test() in R defaults to Welch's t-test (var.equal = FALSE)
# If variances are equal, set var.equal = TRUE
t_result <- t.test(Return ~ Bond, data = bondreturn, var.equal = TRUE)
print(t_result) #p-value = 0.04611


##1.1 - 3


# 1. Load Data
library(sasLM)
sasLM::bondreturn

# 2. Split Data by Group (bank vs corp)
# Extract 'Return' values for each group
group_bank <- bondreturn$Return[bondreturn$Bond == "Bank"]
group_corp <- bondreturn$Return[bondreturn$Bond == "Corp"]

# 3. Calculate Basic Statistics (n, Mean, Variance)
n1 <- length(group_bank)        # Sample size of group 1
n2 <- length(group_corp)        # Sample size of group 2

mean1 <- mean(group_bank)       # Mean of group 1
mean2 <- mean(group_corp)       # Mean of group 2

var1 <- var(group_bank)         # Variance of group 1
var2 <- var(group_corp)         # Variance of group 2

# 4. Calculate Pooled Variance (Sp^2)
# This is used when assuming equal variances
sp2 <- ((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2)

# 5. Calculate t-statistic
# Formula: (Mean1 - Mean2) / sqrt(Sp^2 * (1/n1 + 1/n2))
se_diff <- sqrt(sp2 * (1/n1 + 1/n2))
t_val <- (mean1 - mean2) / se_diff

# 6. Calculate Degrees of Freedom (df) and p-value
df <- n1 + n2 - 2
# Two-tailed p-value using the t-distribution
p_val <- 2 * pt(abs(t_val), df, lower.tail = FALSE)

# 7. Print Results
cat("--- Manual t-test Results ---\n")
cat("t-statistic:", t_val, "\n")
cat("Degrees of Freedom:", df, "\n")
cat("p-value:", p_val, "\n")

# 8. Verification
# Compare with the built-in t.test function
t.test(Return ~ Bond, data = bondreturn, var.equal = TRUE)


##1.1 - 4


# 1. Load library and data
library(sasLM)
sasLM::bondreturn

# 2. Check group levels and sample size
# Verify that there are exactly two groups: 'bank' and 'corp'
table(bondreturn$Bond)

# 3. Check for Homogeneity of Variance (Levene's Test)
# This determines whether to use a standard t-test or Welch's t-test
var.test(Return ~ Bond, data = bondreturn) #p-value = 0.7939

# 4. Perform Independent t-test
t_result <- t.test(Return ~ Bond, 
                   data = bondreturn, 
                   var.equal = TRUE,              # Based on var.test p-value (0.7939)
                   alternative = "greater")          # Testing if bank > corp (i.e., bank - corp > 0)
print(t_result)


##1.1 - 5


# 1. Load library and data
library(sasLM)
sasLM::bondreturn

# 2. Fit the ANOVA model
# Dependent variable: Return, Independent variable: Bond
# Null Hypothesis (H0): Mean returns of 'bank' and 'corp' are equal
anova_model <- aov(Return ~ Bond, data = bondreturn)

# 3. Display the ANOVA table
# Check 'Pr(>F)' for statistical significance
summary(anova_model)

# 4. Check for Group Means
# Provides mean values for each group to understand the direction of difference
model.tables(anova_model, type = "means")

t_result$statistic^2 #5.179293 



##1.2, ##1.3


# 1. Load data
library(sasLM)
sasLM::bondreturn

# 2. Original ANOVA
# Testing original Return values
model_orig <- aov(Return ~ Bond, data = bondreturn)
summary(model_orig)

# 3. ANOVA with Constant Added (Return + 100)
# Location shift: Means change, but variances stay the same
model_add <- aov(I(Return + 100) ~ Bond, data = bondreturn)
summary(model_add)

# 4. ANOVA with Constant Multiplied (Return * 10)
# Scaling: Both MS_between and MS_within are scaled by 10^2
model_mul <- aov(I(Return * 10) ~ Bond, data = bondreturn)
summary(model_mul)


##1.5


# 1. Load the required library and dataset
library(sasLM)
sasLM::bulblife
# 2. Inspect the data structure
# The dataset contains 'Product' (Group) and 'Duration' (Response variable)
str(bulblife)
table(bulblife$Product)

# 3. Perform One-way ANOVA
# Null Hypothesis (H0): There is no difference in the mean duration between Product A and B.
# Alternative Hypothesis (H1): There is a significant difference in the mean duration.
fit <- aov(Duration ~ Product, data = bulblife)

# 4. Display the ANOVA table
# Check the p-value (Pr(>F)) to determine statistical significance
summary(fit)

# 5. Calculate group means
# This shows the average duration for each product type
model.tables(fit, type = "means")


# 6. Visualization without Warnings
# We map colors specifically to 'A' and 'B' to match the 'Product' levels
ggplot(bulblife, aes(x = Product, y = Duration, fill = Product)) +
  geom_boxplot(alpha = 0.7, outlier.shape = NA) +
  geom_jitter(width = 0.1, color = "black", alpha = 0.5) +
  scale_fill_manual(values = c("A" = "#69b3a2", "B" = "#404080")) +
  theme_minimal() +
  labs(
    title = "Analysis of Bulb Duration",
    subtitle = "p-value = 0.121 (Not Significant)",
    x = "Product Type",
    y = "Duration"
  ) +
  theme(legend.position = "none")



##### part 2 -----------------------------------------------------------------------------------------------------------------------



#example

#simple-linear regression
library(tidyverse)
library(sasLM)
str(simxy1)

lm_simxy1 <- lm(Y ~ X, data = simxy1)
summary(lm_simxy1)

windows()
pB(Y ~ X, Data = simxy1)


#simple-linear regression - visualization
# 1. Check data and create a model
library(sasLM)
data <- simxy1
lm_model <- lm(Y ~ X, data = data)

# 2. Generate sorted X values for smooth interval lines
new_x <- seq(min(data$X), max(data$X), length.out = 100)

# 3. Calculate Confidence and Prediction intervals
conf_interval <- predict(lm_model, newdata = data.frame(X = new_x), interval = "confidence")
pred_interval <- predict(lm_model, newdata = data.frame(X = new_x), interval = "prediction")

# 4. Draw basic scatter plot
windows()
plot(data$X, data$Y, 
     pch = 16,               # Point character (filled circle)
     col = "gray",           # Point color
     main = "Simple Linear Regression (Base R Plot)",
     xlab = "X", ylab = "Y",
     ylim = c(-2, 10),       # Set y-axis limits from -2 to 10
     yaxt = "n")             # Suppress default y-axis to customize ticks

# Add custom y-axis with an interval of 2
axis(2, at = seq(-2, 10, by = 2))

# 5. Add regression line
abline(lm_model, col = "blue", lwd = 2)

# 6. Add Confidence Interval (Blue dashed lines)
lines(new_x, conf_interval[, "lwr"], col = "blue", lty = 2)
lines(new_x, conf_interval[, "upr"], col = "blue", lty = 2)

# 7. Add Prediction Interval (Red dashed lines)
lines(new_x, pred_interval[, "lwr"], col = "red", lty = 2)
lines(new_x, pred_interval[, "upr"], col = "red", lty = 2)

# 8. Add legend
legend("bottomright", 
       legend = c("Regression Line", "95% Conf. Int", "95% Pred. Int"),
       col = c("blue", "blue", "red"), 
       lty = c(1, 2, 2), 
       lwd = c(2, 1, 1))


#simple-linear regression - ANOVA
anova(lm_simxy1)
14.4/(14.4 + 2.1)


## exercise

##2.1 

# Line 1: Points (2, 0) and (5, 5)
x1 <- c(2, 5)
y1 <- c(0, 5)

# Line 2: Points (-2, 1) and (6, 6)
x2 <- c(-2, 6)
y2 <- c(1, 6)

# Function to calculate slope and intercept
get_line <- function(x, y) {
  m <- (y[2] - y[1]) / (x[2] - x[1])
  b <- y[1] - m * x[1]
  return(c(m, b))
}

# Calculate parameters
line1_res <- get_line(x1, y1)
line2_res <- get_line(x2, y2)

# Create label strings for the legend
label1 <- paste0("Line 1: y = ", round(line1_res[1], 3), "x + ", round(line1_res[2], 3))
label2 <- paste0("Line 2: y = ", round(line2_res[1], 3), "x + ", round(line2_res[2], 3))

# Print equations to console (as requested)
cat("Line 1 Equation: y =", round(line1_res[1], 3), "x +", round(line1_res[2], 3), "\n")
cat("Line 2 Equation: y =", round(line2_res[1], 3), "x +", round(line2_res[2], 3), "\n")

# Visualization
plot(NULL, xlim = c(-3, 7), ylim = c(-2, 7), 
     xlab = "X-axis", ylab = "Y-axis", main = "Lines with Equations in Legend")

# Plot Line 1
abline(a = line1_res[2], b = line1_res[1], col = "red", lwd = 2)
points(x1, y1, col = "red", pch = 19)

# Plot Line 2
abline(a = line2_res[2], b = line2_res[1], col = "blue", lwd = 2, lty = 2)
points(x2, y2, col = "blue", pch = 17)

# Add Legend with Equations
legend("topleft", legend = c(label1, label2),
       col = c("red", "blue"), lty = c(1, 2), pch = c(19, 17), cex = 0.8)

grid()


##2.3


# Create the data frame
df <- data.frame(
  X = c(0.5, 1.0, 1.5),
  Y = c(2, 1, 3)
)

# Define parameters for Line A and B
m_a <- -1; b_a <- 3
m_b <- 3;  b_b <- -1

# Calculate Sum of Squared Errors (SSE)
pred_a <- m_a * df$X + b_a
sse_a  <- sum((df$Y - pred_a)^2)

pred_b <- m_b * df$X + b_b
sse_b  <- sum((df$Y - pred_b)^2)

# Linear Regression (Best fit line)
fit <- lm(Y ~ X, data = df)
sse_reg <- sum(residuals(fit)^2)

# Visualization setup
par(mar = c(5, 4, 4, 2))

# Plot the points
plot(df$X, df$Y, 
     pch = 19, 
     xlim = c(0, 2), 
     ylim = c(-1, 5), 
     xlab = "X Label", 
     ylab = "Y Label", 
     main = "Error Analysis and Regression Line")

# Restrict line drawing to the plot area
clip(0, 2, -1, 5)

# 1. Plot Line A & B as solid lines (lty = 1)
abline(a = b_a, b = m_a, col = "red", lwd = 2, lty = 1)
abline(a = b_b, b = m_b, col = "blue", lwd = 2, lty = 1)

# 2. Plot Regression Line as a dashed line (lty = 2)
abline(fit, col = "darkgreen", lwd = 2, lty = 2)

# Add Legend with adjusted spacing and line types
legend("topleft", 
       inset = c(0.05, 0.05), # Move further from the edge to prevent clipping
       legend = c("Data Points", 
                  paste("Line A (SSE:", sse_a, ")"), 
                  paste("Line B (SSE:", sse_b, ")"),
                  paste("Regression (SSE:", round(sse_reg, 2), ")")),
       col = c("black", "red", "blue", "darkgreen"), 
       pch = c(19, NA, NA, NA), 
       lty = c(NA, 1, 1, 2),  # Match the line styles: Solid, Solid, Dashed
       cex = 0.8,             # Font size
       y.intersp = 1.5,       # Increase vertical space between legend items
       bg = "white",          # White background to cover grid lines
       box.col = "black")     # Clear border color

# Add background grid
grid()


##2.4


# 1. Data Frame Setup
df <- data.frame(
  Year = c(1980, 1982, 1985, 1987, 1989),
  X = c(430, 395, 360, 270, 180),
  Y = c(40, 60, 80, 88, 98)
)

# 2. Simple Linear Regression
model <- lm(Y ~ X, data = df)

# 3. Visualization Setup
# Increase right margin slightly to ensure legend has enough space
par(mar = c(5, 4, 4, 3))

# Define axis limits for consistency
x_min <- 150; x_max <- 450
y_min <- 30;  y_max <- 110

# Plot the scatter points
plot(df$X, df$Y,
     pch = 19,
     col = "blue",
     xlim = c(x_min, x_max),
     ylim = c(y_min, y_max),
     main = "Error Analysis and Regression Line",
     xlab = "Independent Variable (X)",
     ylab = "Dependent Variable (Y)")

# Add background grid
grid()

# 4. Restrict Line Drawing to the Plot Area
# Ensures abline does not leak outside the axes
clip(x_min, x_max, y_min, y_max)

# Add the regression line (Dashed)
abline(model, col = "red", lwd = 2, lty = 2)

# 5. Adjusted Legend to Prevent Clipping
# Reset clip to allow drawing the legend properly if needed
do.call(clip, as.list(par("usr"))) 

legend("topright",
       # inset: move legend away from the top-right corner edges
       inset = c(0.05, 0.05), 
       legend = c("Data Points", "Regression Line (Dashed)"),
       col = c("blue", "red"),
       pch = c(19, NA),
       lty = c(NA, 2),
       lwd = c(NA, 2),
       cex = 0.8,           # Font size
       y.intersp = 1.5,      # Vertical spacing between items
       bg = "white",         # White background to prevent grid overlap
       box.col = "black")    # Explicit border to see the legend clearly

# 6. Statistical Outputs (Console)
print("--- Linear Regression Summary ---")
print(summary(model))

print("--- ANOVA Table ---")
print(anova(model))

model_anova <- anova(model)
model_anova$`Sum Sq`[1] #SSR
model_anova$`Sum Sq`[2] #SSE




#exercise 3.4 (1)

# 1. Data Input
male <- c(327, 291, 323, 284, 305)
female <- c(308, 324, 353, 344, 341)

# Combine into a data frame
# English Comment: Create a categorical variable for gender (0 for male, 1 for female)
value <- c(male, female)
gender <- factor(x = c(rep("Male", 5), rep("Female", 5)))
gender <- ifelse(gender == "Male", -1, 1)
df <- data.frame(gender, value)

# 3. Simple Regression Analysis
# English Comment: Fit a linear model. Female is usually the reference group by default (alphabetical).
fit_lm <- lm(value ~ gender, data = df)
cat("\n--- Regression Analysis Summary ---\n")
print(summary(fit_lm))

# 4. Analysis of Variance (ANOVA)
# English Comment: Perform ANOVA to see the group differences.
fit_aov <- aov(value ~ gender, data = df)
cat("\n--- ANOVA Table ---\n")
print(summary(fit_aov))



##### part 3 -----------------------------------------------------------------------------------------------------------------------



#exercise 3.4 (2)


# Data Input
male_val <- c(327, 291, 323, 284, 305)
female_val <- c(308, 324, 353, 344, 341)

# Case 1: If original x was -1 and 1
m <- -1
l <- 2
x_orig <- c(-1, 1) # Male, Female
x_star <- (x_orig - m) / l
# Result will be 0, 1

# Case 2: Mapping categorical data to 0 and 1 in a model
# English Comment: Create a dummy variable where Male = 0 and Female = 1
gender_factor <- factor(c(rep("Male", 5), rep("Female", 5)), levels = c("Male", "Female"))
x_dummy <- as.numeric(gender_factor) - 1 

# English Comment: This x_dummy will be 0 for Male and 1 for Female.
print(x_dummy)


#exercise 3.4 (3)
df_2 <- data.frame(x_dummy, value)
fit_lm_2 <- lm(value ~ x_dummy,, data = df_2)
fit_aov_2 <- aov(value ~ x_dummy,, data = df_2)

summary(fit_lm_2)
summary(fit_aov_2)



##### part 4 -----------------------------------------------------------------------------------------------------------------------



#exercise 4.5


library(sasLM)
edupgm

aov_edupgm <- aov(formula = Sales ~ Program , data = edupgm)
summary(aov_edupgm)

#Create the boxplot
# Using formula Sales ~ Program to see distribution by group
boxplot(Sales ~ Program, 
        data = edupgm, 
        main = "Sales Distribution by Education Program", # Title
        xlab = "Education Program",                       # X-axis label
        ylab = "Sales Amount",                           # Y-axis label
        col = c("lightblue", "lightgreen", "lightpink"), # Box colors
        border = "darkgray",                             # Border color
        notch = FALSE)                                   # Add notch if needed

GLM(Formula = Sales ~ Program , Data = edupgm)

aov3(Formula = Sales ~ Program , Data = edupgm)

summary(aov_edupgm)
library(car)
mean(edupgm$Sales)
Anova(aov_edupgm, type = 3)


#intercept - calculate
library(sasLM)
y <- edupgm$Sales
X <- model.matrix(Sales ~ Program, data = edupgm)

# 1. Estimate coefficients (Beta hat)
# Beta_hat = (X'X)^-1 * X'Y
XtX_inv <- solve(t(X) %*% X)
beta_hat <- XtX_inv %*% t(X) %*% y

# 2. Set Hypothesis Matrix L for Intercept
# L picks the first coefficient (Intercept)
L <- matrix(c(1, 0, 0, 0), nrow = 1)

# 3. Apply the Quadratic Form for Type III SS
# SS = (L*b)' * [L*(X'X)^-1*L']^-1 * (L*b)
ss_intercept_direct <- t(L %*% beta_hat) %*% solve(L %*% XtX_inv %*% t(L)) %*% (L %*% beta_hat)

# Verification
cat("Direct Matrix Calculation Result:", as.numeric(ss_intercept_direct), "\n")
# This will result in exactly 27676.8


#estimate beta

# Sample Data
y <- c(10, 12, 20, 22)
group <- factor(rep(c("A", "B"), each = 2))

# 1. Treatment Coding (G1 방식)
fit_treatment <- lm(y ~ group, contrasts = list(group = contr.treatment))
model.matrix(fit_treatment)
y_hat_1 <- predict(fit_treatment)
fit_treatment
# 2. Sum Coding (G2 방식)
fit_sum <- lm(y ~ group, contrasts = list(group = contr.sum))
model.matrix(fit_sum)
y_hat_2 <- predict(fit_sum)
fit_sum
# Compare the results
print("Predictions from Treatment Coding:")
print(y_hat_1)

print("Predictions from Sum Coding:")
print(y_hat_2)

# Check if they are exactly the same
print(paste("Are the two shadows identical?", all.equal(y_hat_1, y_hat_2)))


#confidence interval
library(emmeans)
pgm_means <- emmeans(aov_edupgm, "Program")
summary(pgm_means, infer = TRUE)
n <-nrow(model.matrix(aov_edupgm))
pgm_diff <- pairs(pgm_means)
confint(pgm_diff)

# 1. Extract MSE and degrees of freedom for error from ANOVA summary
res <- summary(aov_edupgm)[[1]]
mse <- res["Residuals", "Mean Sq"]
df_e <- res["Residuals", "Df"]

# 2. Calculate t-critical value (Two-tailed, 95% confidence)
alpha <- 0.05
t_val <- qt(1 - alpha/2, df_e)

# 3. Target Group Setup (e.g., Program 'A')
# It is crucial to use the sample size of the specific group (ni), not the total N
target_group <- 1
mean_a <- mean(edupgm$Sales[edupgm$Program == target_group]) # Group Mean
ni <- sum(edupgm$Program == target_group)                    # Group sample size

# 4. Calculate Standard Error (SE) for the group mean
# Logic: SE = sqrt( MSE / ni )
se_a <- sqrt(mse / ni)

# 5. Calculate Confidence Interval
lower_ci <- mean_a - t_val * se_a
upper_ci <- mean_a + t_val * se_a

# Print results for verification
cat("--- Manual Calculation Results ---\n")
cat("Group:", target_group, "\n")
cat("Mean:", mean_a, "\n")
cat("n_i:", ni, "\n")
cat("95% CI:", lower_ci, "to", upper_ci, "\n")


#exercise 4.10


#2.1
library(sasLM)
transfat

aov_transfat <- aov(formula = Trans ~ Fat, data = transfat)
summary(aov_transfat)

library(car)
Anova(aov_transfat, type = 3)

#2.2
library(emmeans)
transfat_CI <- emmeans(object = aov_transfat, specs = "Fat")
summary(transfat_CI)
aggregate(Trans ~ Fat, data = transfat, FUN = "mean")

#2.3
transfat_CI_diff <- pairs(transfat_CI)
confint(transfat_CI_diff )

#2.4
transfat <- transfat %>%
  mutate(Source = case_when(
    Fat %in% c(1, 2) ~ "Vegetable",
    Fat %in% c(3, 4) ~ "Animal",
    TRUE ~ NA_character_ # Handle unexpected levels
  ))
aov_transfat_2 <- aov(formula = Trans ~ Source, data = transfat)
transfat_CI_2 <- emmeans(object = aov_transfat_2, specs = "Source") 
pairs(transfat_CI_2)  %>% 
  confint()


#exercise 4.5

#4.5
library(sasLM)
brandtar

#1
plot(brandtar)

#2
brandtar_mean <- aggregate(x = Tar ~ Brand, data = brandtar, FUN = "mean");brandtar_mean

Carlton <- brandtar %>% 
  filter(Brand == "Carlton") 
t.test(x = Carlton$Tar, mu = 0.2, alternative = "less") #significant

Now <- brandtar %>%
  filter(Brand == "Now") 
t.test(x = Now$Tar, mu = 0.2, alternative = "less") #non-significant

Cambridge <- brandtar %>%
  filter(Brand == "Cambridge") 
t.test(x = Cambridge$Tar, mu = 0.2, alternative = "less") #non-significant

#3
brandtar_aov <- aov(formula = Tar ~ Brand, data = brandtar)
summary(brandtar_aov)
library(car)
Anova(brandtar_aov)

#4
library(emmeans)
brandtar_aov_CI <- emmeans(object = brandtar_aov, specs = "Brand")
pairs(brandtar_aov_CI) %>% 
  confint()



##### part 5 -----------------------------------------------------------------------------------------------------------------------



#example - post-hoc test 


#LSD
library(sasLM)
transfat

anova_transfat <- aov(formula = Trans ~ Fat, data = transfat)
summary(anova_transfat)


library(agricolae)
lsd_test <- LSD.test(anova_transfat, "Fat", p.adj="none", console=TRUE)
lsd_test

plot(lsd_test, main = "LSD Test: Trans Fat by Fat Type")


#bonferroni
bonferroni_out <- LSD.test(anova_transfat, "Fat", p.adj = "bonferroni",  console=TRUE)
bonferroni_out

plot(bonferroni_out, main = "Bonferroni Corrected Post-hoc Test")


#Tukey
str(transfat)
table(transfat$Fat)

tukey_out <- HSD.test(anova_transfat, "Fat", group = TRUE)
tukey_out
plot(tukey_out, main = "Tukey's HSD Test: Trans Fat Content")

TukeyHSD(anova_transfat)


#Scheffe
scheffe_out <- scheffe.test(anova_transfat, "Fat", group = TRUE)
scheffe_out
plot(scheffe_out, main = "Scheffe's Test: Trans Fat Content")


#Duncan
duncan_out <- duncan.test(anova_transfat, "Fat", group = TRUE)
duncan_out
plot(Trans ~ Fat, data = transfat, main = "Trans Fat Content by Fat Type")

#dunet
# Set graphical parameters: 1 row, 3 columns
par(mfrow = c(1,3))

library(multcomp)
library(sasLM)

# --- Test 1: Control = Fat "1" ---
transfat$Fat <- relevel(factor(transfat$Fat), ref = "1")
# MUST re-fit the model after releveling
anova_transfat.1 <- aov(Trans ~ Fat, data = transfat) 
dunnett_result.1 <- glht(anova_transfat.1, linfct = mcp(Fat = "Dunnett"))
summary(dunnett_result.1)
plot(dunnett_result.1, main = "Control = 1")

# --- Test 2: Control = Fat "2" ---
transfat$Fat <- relevel(factor(transfat$Fat), ref = "2")
anova_transfat.2 <- aov(Trans ~ Fat, data = transfat)
dunnett_result.2 <- glht(anova_transfat.2, linfct = mcp(Fat = "Dunnett"))
summary(dunnett_result.2)
plot(dunnett_result.2, main = "Control = 2")

# --- Test 3: Control = Fat "3" ---
transfat$Fat <- relevel(factor(transfat$Fat), ref = "3")
anova_transfat.3 <- aov(Trans ~ Fat, data = transfat)
dunnett_result.3 <- glht(anova_transfat.3, linfct = mcp(Fat = "Dunnett"))
summary(dunnett_result.3)
plot(dunnett_result.3, main = "Control = 3")

# Reset plotting layout
par(mfrow = c(1,1))


#post-hoc test All

# Load required libraries
library(sasLM)
library(agricolae)
library(multcomp)
library(ggplot2)
library(dplyr)
library(patchwork)

# 1. Prepare Data and Relevel for Dunnett's Test
transfat$Fat <- as.factor(transfat$Fat)

# Set Fat "2" as the reference level (Control Group)
transfat$Fat <- relevel(transfat$Fat, ref = "2")

# Fit the ANOVA model with the new reference level
anova_model <- aov(Trans ~ Fat, data = transfat)

# --- 2. Define Helper Function for Bar Plots (Grouping Letters) ---
create_bar_plot <- function(res_object, title) {
  # Extract group letters and means
  df <- res_object$groups
  df$Fat <- rownames(df)
  colnames(df)[1:2] <- c("Mean", "Letter")
  
  # Reorder Fat for consistent x-axis (optional)
  df$Fat <- factor(df$Fat, levels = levels(transfat$Fat))
  
  ggplot(df, aes(x = Fat, y = Mean, fill = Fat)) +
    geom_bar(stat = "identity", color = "black", alpha = 0.7) +
    geom_text(aes(label = Letter), vjust = -0.5, size = 3) +
    labs(title = title, x = NULL, y = NULL) +
    theme_minimal(base_size = 9) +
    theme(legend.position = "none")
}

# --- 3. Generate 5 Bar Plots (LSD, Bonferroni, HSD, Scheffe, Duncan) ---
# Note: These methods compare all pairs, so the 'ref' level doesn't change the grouping results
p1 <- create_bar_plot(LSD.test(anova_model, "Fat", group = TRUE), "LSD")
p2 <- create_bar_plot(LSD.test(anova_model, "Fat", p.adj="bonferroni", group = TRUE), "Bonferroni")
p3 <- create_bar_plot(HSD.test(anova_model, "Fat", group = TRUE), "Tukey HSD")
p4 <- create_bar_plot(scheffe.test(anova_model, "Fat", group = TRUE), "Scheffe")
p5 <- create_bar_plot(duncan.test(anova_model, "Fat", group = TRUE), "Duncan")

# --- 4. Generate Dunnett Plot (Specific Comparison to Control = Fat 2) ---
# glht uses the first level of the factor as control (already set to "2" via relevel)
dunnett_res <- glht(anova_model, linfct = mcp(Fat = "Dunnett"))
dunnett_conf <- as.data.frame(confint(dunnett_res)$confint)
dunnett_conf$Comparison <- rownames(dunnett_conf)

p6 <- ggplot(dunnett_conf, aes(x = Comparison, y = Estimate)) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  geom_point(size = 2) +
  geom_errorbar(aes(ymin = lwr, ymax = upr), width = 0.2) +
  coord_flip() +
  labs(title = "Dunnett (vs Fat 2)", x = NULL, y = "Diff") +
  theme_light(base_size = 9)

# --- 5. Combine All 6 Plots using patchwork ---
# Layout: 2 rows and 3 columns
combined_report <- (p1 | p2 | p3) / (p4 | p5 | p6)

combined_report + 
  plot_annotation(
    title = "Post-hoc Analysis with Control = Fat 2",
    subtitle = "Integration of 6 Methods using Patchwork",
    caption = "Reference group for Dunnett is Fat 2. Bars show means with significance letters."
  )



#diffogram


if(!require(HH)) install.packages("HH")
library(HH)

# 1. Fit the ANOVA model
# Using the transfat data or similar factorial data
data(transfat)
transfat$Fat <- as.factor(transfat$Fat)
anova_model <- aov(Trans ~ Fat, data = transfat)

# 2. Calculate Glht (General Linear Hypotheses) for all pairs (Tukey)
library(multcomp)
posthoc_all <- glht(anova_model, linfct = mcp(Fat = "Tukey"))

# 3. Create a Diffogram (MMC Plot)
# The mmc function prepares the data for the diffogram
fat_mmc <- mmc(anova_model, focus = "Fat")

# Plotting the Diffogram
plot(fat_mmc, type = "diffogram", 
     main = "Diffogram of Fat Types (Tukey HSD)")



#contrast and trend



# Standard dataset example
library(sasLM)
transfat

# 2. Convert 'Fat' to an Ordered Factor
# This is crucial for R to apply polynomial contrasts (Linear, Quadratic, etc.)
transfat$Fat <- factor(transfat$Fat, levels = c("1", "2", "3", "4"), ordered = TRUE)

# 3. Check the internal orthogonal coefficients
# R automatically assigns coefficients like (-3, -1, 1, 3) for 4 levels
print("--- Orthogonal Polynomial Contrast Coefficients ---")
print(contrasts(transfat$Fat))

# 4. Fit ANOVA model
# 'aov' handles the partitioning of variance
fit_trend <- aov(Trans ~ Fat, data = transfat)

# 5. Extract Detailed Trend Results
# 'split' argument allows us to see individual P-values for Linear and Quadratic components
trend_summary <- summary(fit_trend, 
                         split = list(Fat = list("Linear" = 1, "Quadratic" = 2, "Cubic" = 3)))

print(trend_summary)

# 6. Visualization of the Trend
library(ggplot2)

ggplot(transfat, aes(x = Fat, y = Trans, group = 1)) +
  stat_summary(fun = base::mean, geom = "point", size = 4, color = "darkblue") +
  stat_summary(fun = base::mean, geom = "line", linetype = "dashed", color = "red") +
  stat_summary(fun.data = mean_se, geom = "errorbar", width = 0.1) +
  labs(title = "Trend Analysis: Visualizing Linear & Quadratic Components",
       subtitle = "The red dashed line shows the connected means of each group",
       x = "Fat Type (Ordered)", 
       y = "Trans Fat Content") +
  theme_minimal()


# 1. Generate the full 3x3 orthonormal matrix
# The first row is the normalized grand mean
row1 <- rep(1/sqrt(3), 3)

# The next two rows are from R's polynomial contrasts (already normalized)
poly_part <- t(contr.poly(3))

# Combine them into matrix M
M_3 <- rbind(row1, poly_part)
rownames(M_3) <- c("Mean", "Linear", "Quadratic")

# 2. Print the matrix
print(M_3)

# 3. Verify that M * t(M) is an Identity Matrix
# This confirms all rows are unit vectors and orthogonal to each other
print(round(M_3 %*% t(M_3), 10))



#Main plot Effect



# Load required libraries
library(sasLM)
library(ggplot2)
library(dplyr)
# install.packages("patchwork")
library(patchwork)

data(transfat)

# ---------------------------------------------------------
# Plot 1: Individual SE (Based on group-specific standard deviation)
# ---------------------------------------------------------
df_individual <- transfat %>%
  group_by(Fat) %>%
  summarise(
    mean_val = mean(Trans),
    se = sd(Trans) / sqrt(n()), # Individual variance approach
    .groups = 'drop'
  )

p1 <- ggplot(df_individual, aes(x = reorder(Fat, mean_val), y = mean_val, group = 1)) +
  geom_point(size = 3) +
  geom_line(color = "red", linetype = "dashed") + 
  geom_errorbar(aes(ymin = mean_val - se, ymax = mean_val + se), width = 0.1) +
  labs(title = "1. Individual SE Plot",
       subtitle = "Calculated using group-specific sd()",
       x = "Fat (Ordered by Mean)", y = "Trans Mean") +
  theme_bw()

# ---------------------------------------------------------
# Plot 2: Pooled SE (Based on ANOVA's Shared MSE)
# ---------------------------------------------------------
fit <- aov(Trans ~ Fat, data = transfat)
mse <- summary(fit)[[1]]["Residuals", "Mean Sq"] # Root Mean Square Error squared

df_pooled <- transfat %>%
  group_by(Fat) %>%
  summarise(
    mean_val = mean(Trans),
    n = n(),
    .groups = 'drop'
  ) %>%
  mutate(
    se = sqrt(mse / n) # Mathematical pooled SE from ANOVA model
  )

p2 <- ggplot(df_pooled, aes(x = reorder(Fat, mean_val), y = mean_val, group = 1)) +
  geom_point(size = 3) +
  geom_line(color = "blue", size = 1) + 
  geom_errorbar(aes(ymin = mean_val - se, ymax = mean_val + se), width = 0.1, color = "darkblue") +
  labs(title = "2. Pooled SE Plot (ANOVA Assumption)",
       subtitle = "Calculated using model's shared MSE",
       x = "Fat (Ordered by Mean)", y = "Trans Mean") +
  theme_bw()

# ---------------------------------------------------------
# Combine plots with patchwork
# ---------------------------------------------------------
# Using '+' for side-by-side, '|' also works
# plot_annotation adds a global title to the combined figure
combined_plot <- (p1 | p2) + 
  plot_annotation(
    title = 'Comparison of Error Bar Calculation Methods',
    subtitle = 'Left: Non-ANOVA (Heteroscedastic) | Right: ANOVA (Homoscedastic) Logic',
    caption = 'Note: The Pooled SE matches the results from sasLM::LSM()'
  )

# Display the result
combined_plot



##exercise 



##5.1
library(sasLM)
pigfeed
str(pigfeed)
pigfeed$Feed

#1
aov_pigfeed <- aov(formula = Gain ~ Feed, data = pigfeed)
summary(aov_pigfeed)

model.matrix(aov_pigfeed)
aggregate(x = Gain ~ Feed, data = pigfeed, FUN = "mean")
mean(pigfeed$Gain)
coef(aov_pigfeed)

#2
library(car)
Anova(mod = aov_pigfeed, type = 3)

#3
fit_full <- lm(Gain ~ Feed, data = pigfeed)
fit_reduced <- lm(Gain ~ 1, data = pigfeed)
comparison <- anova(fit_reduced, fit_full)
print(comparison)

#4
library(agricolae)
lsd_test <- LSD.test(aov_pigfeed, "Feed", p.adj="none", console=TRUE)

plot(lsd_test, main = "LSD Test: Gain by Feed Type")

#5
hsd_test <- HSD.test(aov_pigfeed, "Feed", group = TRUE, console=TRUE)
plot(hsd_test, main = "Tukey's HSD Test: Gain by Feed Type")

#6
scheffe_test <- scheffe.test(aov_pigfeed, "Feed", group = TRUE, console=TRUE)
plot(scheffe_test, main = "Scheffe's Test: Gain by Feed Type")

#7
levels(pigfeed$Feed)
library(multcomp)
dunnet_test <- glht(aov_pigfeed, linfct = mcp(Feed = "Dunnett"))
summary(dunnet_test)
plot(dunnet_test, main = "Control = 1")


##5.2
library(sasLM)
hlpump
str(hlpump)

#1 one-way

#2
attach(hlpump)
aov_hlpump <- aov(Blood ~ Speed, data = hlpump)
summary(aov_hlpump)

library(car)
Anova(mod = aov_hlpump, type = "3")

#3
boxplot(Blood ~ Speed, data = hlpump)

#4
summary(aov_hlpump, 
        split = list(Speed = list("Linear" = 1, 
                                  "Quadratic" = 2, 
                                  "Cubic" = 3,
                                  "Quartic" = 4)))

#5
install.packages('emmeans')
library(emmeans)
aov_hlpump_CI <- emmeans(object = aov_hlpump, specs = "Speed")
library(tidyverse)
pairs(aov_hlpump_CI) %>% 
  confint()

#6
aov_hlpump_CI
aggregate(Blood ~ Speed, data = hlpump, FUN = "mean")


##5.3

#1 one-way

#2
library(sasLM)
BP
str(BP)

aov_BP <- aov(DeltaBP ~ Method, data = BP)
summary(aov_BP)

#3 H0, H1

#4
BP_Exercise <- BP %>% 
  filter(Method == "Exercise") 
BP_Diet <- BP %>% 
  filter(Method == "Diet") 
t.test(BP_Exercise$DeltaBP,BP_Diet$DeltaBP)

#5
library(agricolae)
lsd_test <- LSD.test(aov_BP, "Method", p.adj="none", console=TRUE)
plot(lsd_test, main = "LSD Test: DeltaBP by Method Type")




##### part 6 -----------------------------------------------------------------------------------------------------------------------



#example


#F-test
library(sasLM)
score1
var_gendr <- aggregate(Y ~ X, data = score1, FUN = "var")
table(score1$X)
pf(q = var_gendr[1,2]/var_gendr[2,2], df1 = 5-1, df2 = 5-1)


#Bartlett's Test
library(sasLM)
edupgm
str(edupgm)
bartlett.test(formula = Sales ~ Program, data = edupgm)

edupgm_var <- tapply(X = edupgm$Sales, INDEX = edupgm$Program, FUN = var)
edupgm_length <- tapply(X = edupgm$Sales, INDEX = edupgm$Program, FUN = length)
a <- length(edupgm_var)
N <- sum(edupgm_length)

sum( (edupgm_length - 1)*edupgm_var ) / (N-a)


#normality test
library(sasLM)
score1

boxplot(score1)
library(tidyverse)
score1 %>% 
  filter(X == -1) %>% 
  .$Y %>% 
  hist()
library(tidyverse)
score1 %>% 
  filter(X == 1) %>% 
  .$Y %>% 
  hist()  

par(mfrow = c(1,2))
# 1. existing code
n <- length(score1$Y)
# Calculating theoretical quantiles (qi)
z <- qnorm((1:n - 0.5)/n)
# Plotting: Observed (x) vs Theoretical (y)
plot(x = sort(score1$Y), y = z, 
     xlab = "Observed Values (Sorted Y)", 
     ylab = "Theoretical Quantiles (z)",
     main = "Manual QQ Plot with Reference Line")

# 2. Adding the equivalent of 'qqline'
# Step: Find Q1 and Q3 for both axes to draw a robust line
y_q13 <- quantile(score1$Y, c(0.25, 0.75)) # Quantiles of observed data
z_q13 <- qnorm(c(0.25, 0.75))             # Quantiles of theoretical normal
# Calculate slope (a) and intercept (b) based on Q1 and Q3
slope <- diff(z_q13) / diff(y_q13)
intercept <- z_q13[1] - slope * y_q13[1]

# Add the reference line (Red color)
abline(intercept, slope, col = "red", lwd = 2)

qqnorm(score1$Y, main = "Normal Q-Q Plot for score1$Y")
qqline(score1$Y, col = "red", lwd = 2) # Add a reference line


shapiro.test(score1$Y)
library(nortest)
# Assuming score1$Y is your data from the previous image
data_to_test <- score1$Y
# 1. Kolmogorov-Smirnov Test (Lilliefors correction for normality)
# Standard ks.test requires mean and sd, but lillie.test is for normality
ks_res <- lillie.test(data_to_test)
# 2. Cramér-von Mises Normality Test
cvm_res <- cvm.test(data_to_test)
# 3. Anderson-Darling Normality Test
ad_res <- ad.test(data_to_test)

# Print results
print(ks_res)
print(cvm_res)
print(ad_res)

#visualization
library(ggplot2)
library(nortest)
library(patchwork)

# 1. Compute all p-values
p_sw <- shapiro.test(score1$Y)$p.value
p_ad <- ad.test(score1$Y)$p.value
p_cvm <- cvm.test(score1$Y)$p.value
p_ks <- lillie.test(score1$Y)$p.value

# 2. Labels for plots
results_label <- paste0(
  "Normality Test Results (p-values):\n",
  "----------------------------------\n",
  "Shapiro-Wilk:      ", sprintf("%.4f", p_sw), "\n",
  "Anderson-Darling:  ", sprintf("%.4f", p_ad), "\n",
  "Cramér-von Mises:  ", sprintf("%.4f", p_cvm), "\n",
  "Lilliefors (KS):   ", sprintf("%.4f", p_ks)
)

# Annotation for the histogram plot
hist_legend_label <- "Red line: Normal distribution\nBlue line: Histogram density"

# 3. Create Histogram + Density Plot (p1)
p1 <- ggplot(score1, aes(x = Y)) +
  geom_histogram(aes(y = ..density..), bins = 15, fill = "skyblue", color = "white", alpha = 0.7) +
  geom_density(color = "blue", size = 1) +
  stat_function(fun = dnorm, args = list(mean = mean(score1$Y), sd = sd(score1$Y)), 
                color = "red", linetype = "dashed", size = 1) +
  # Added annotation for p1
  annotate("label", x = -Inf, y = Inf, label = hist_legend_label,
           hjust = 0, vjust = 1, size = 3.5, fill = "white", alpha = 0.8) +
  labs(title = "Distribution of Y with Normal Curve", x = "Value", y = "Density") +
  theme_minimal() +
  scale_y_continuous(expand = expansion(mult = c(0.05, 0.2))) # Give room for label

# 4. Create Q-Q Plot (p2)
p2 <- ggplot(score1, aes(sample = Y)) +
  stat_qq(color = "darkblue") +
  stat_qq_line(color = "red") +
  annotate("label", x = -Inf, y = Inf, label = results_label, 
           hjust = 0, vjust = 1, size = 3.5, family = "mono", fill = "white", alpha = 0.8) +
  labs(title = "Normal Q-Q Plot", x = "Theoretical Quantiles", y = "Sample Quantiles") +
  theme_minimal() +
  scale_y_continuous(expand = expansion(mult = c(0.05, 0.2)))

# Combine plots
p1 + p2



#indepence test
library(sasLM)
hlpump
hlpump_lm <- lm(Blood ~ Speed, data = hlpump)
res <- hlpump_lm$residuals
n <- length(res)
acf(x = res, lag.max = 1, plot = FALSE)

dw <- sum((res[2:n] - res[1:(n-1)])^2) / sum(res^2);dw

library(lmtest)
# H0: rho = 0 (No autocorrelation)
# H1: rho > 0 (Positive autocorrelation) - default
dw_result <- dwtest(hlpump_lm)
print(dw_result)

# 2. To check for both positive and negative autocorrelation
dw_two_sided <- dwtest(hlpump_lm, alternative = "two.sided")
print(dw_two_sided)



#Box-cox
par(mfrow = c(1,1))
library(MASS)
boxcox(hlpump_lm) #CI includes zero; recommend log transformation



#exercise

#1 D = 2(1-rho)

#2

#1)
library(sasLM)
brandtar

brandtar_aov <- aov(Tar ~ Brand, data  = brandtar)
summary(brandtar_aov)
res <- brandtar_aov$residuals

hist(res)

#2)
plot(x = brandtar$Brand, y = res)

#3)
bartlett.test(Tar ~ Brand, data  = brandtar)

# 4) Normal Q-Q Plot (Corrected)
res_sorted <- sort(res) # Sort residuals for the Y-axis
n <- length(res)
z <- qnorm((1:n - 0.5)/n) # Theoretical quantiles for the X-axis

# Plot residuals vs. theoretical quantiles
plot(x = z, y = res_sorted, 
     main = "Normal Q-Q Plot", 
     xlab = "Theoretical Quantiles", 
     ylab = "Sample Residuals")

# Calculate slope and intercept based on the 1st and 3rd quartiles
z_quartile <- qnorm(c(0.25, 0.75))
res_quartile <- quantile(res, c(0.25, 0.75))

slope <- diff(res_quartile) / diff(z_quartile)
intercept <- res_quartile[1] - slope * z_quartile[1]

# Now the abline will match the scale of the plot
abline(a = intercept, b = slope, col = "red", lwd = 2)

library(nortest)
ad.test(res)


#3 ∑∑e=0

#4
sum(res)

#5 box-cox

#6
set.seed(123)
treat <- c(rep(-1,20), rep(1,20))
y <- exp(3 + 1*rnorm(40) + treat)
library(tidyverse)
df <- cbind(treat, y) %>% 
  as.data.frame()
df$treat <- as.factor(df$treat)
plot(y ~ treat, data = df)

df_lm <- lm(y ~ treat, data = df)
res <- df_lm$residuals

hist(res)

qqnorm(res)
qqline(res)

shapiro.test(res)
library(nortest)
ad.test(res)

boxcox(df_lm, data = df)


df_lm_2 <- lm(log(y) ~ treat, data = df)
res.2 <- df_lm_2$residuals

hist(res.2)

qqnorm(res.2)
qqline(res.2)

shapiro.test(res.2)
library(nortest)
ad.test(res.2)

#7 Var(et) = Var(et-1)



##### part 7 -----------------------------------------------------------------------------------------------------------------------



#example


#interaction plot

library(sasLM)
fertpest2

library(tidyverse)
library(magrittr)
A1_means <- fertpest2 %>% 
  filter(A == 1)%$%
  tapply(X = Y, INDEX = B, FUN = mean)
A2_means <- fertpest2 %>% 
  filter(A == 2)%$%
  tapply(X = Y, INDEX = B, FUN = mean)
A1_means;A2_means

library(ggplot2)
library(dplyr)

# Summary table for plotting
inter_df <- fertpest2 %>%
  group_by(A, B) %>%
  summarise(mean_Y = mean(Y), .groups = 'drop')

# ggplot Interaction Plot (X-axis: A, Group: B)
ggplot(inter_df, aes(x = factor(A), y = mean_Y, color = factor(B), group = factor(B))) +
  geom_line(size = 1.2) +
  geom_point(size = 4) +
  labs(title = "Interaction Effect of A and B",
       subtitle = "X-axis represents Factor A",
       x = "Factor A (Fertilizer)",
       y = "Mean of Y",
       color = "Factor B (Pest)") +
  theme_minimal() +
  theme(legend.position = "bottom")


#two-way anova

library(sasLM)
defect1

defect1_aov <- aov(Defects ~ Machine + Employee + Machine:Employee, data = defect1)
summary(defect1_aov)
library(car)
Anova(mod = defect1_aov, type = 3)
Anova(mod = defect1_aov, type = 2)

library(emmeans)
joint_means <- emmeans(object = defect1_aov, specs = ~ Machine | Employee)
print(joint_means)
pairs(joint_means)

library(sasLM)
library(tidyverse)

# 1. Calculate means for plotting
plot_data <- defect1 %>%
  group_by(Machine, Employee) %>%
  summarise(mean_defects = mean(Defects), .groups = 'drop')

# 2. Create the Interaction Plot
ggplot(plot_data, aes(x = factor(Machine), y = mean_defects, 
                      color = factor(Employee), group = factor(Employee))) +
  geom_line(size = 1.2) + 
  geom_point(size = 4) +
  labs(title = "Interaction Plot: Machine vs Employee",
       subtitle = "Checking for non-parallel lines (Interaction)",
       x = "Machine Type",
       y = "Mean of Defects",
       color = "Employee ID") +
  theme_minimal()


#two-way mixed(fixed + random)
library(sasLM)
defect1


library(lmerTest)
aov_mixed <- lmer(formula = Defects ~ Machine + (1 | Employee) + (1 | Machine:Employee), 
                  data = defect1)
summary(aov_mixed)


library(GAD)
Machine_f  <- as.fixed(defect1$Machine)
Employee_r <- as.random(defect1$Employee)
GAD_1 <- lm(Defects ~ Machine_f * Employee_r, data = defect1)
gad(GAD_1)

Machine_r  <- as.random(defect1$Machine)
Employee_f <- as.fixed(defect1$Employee)
GAD_2 <- lm(Defects ~ Machine_r * Employee_f, data = defect1)
gad(GAD_2)


#post-hoc test
library(emmeans)
library(multcomp)
library(multcompView)

int_means <- emmeans(GAD_1, ~ Machine_f * Employee_r)

cld_result <- cld(int_means, Letters = letters, adjust = "sidak", reversed = TRUE)

print(cld_result)

plot_data <- as.data.frame(cld_result)
ggplot(data = plot_data, 
       mapping = aes(x = reorder(interaction(Machine_f, Employee_r), emmean), 
                     y = emmean)) +
  geom_point(size = 3) +
  geom_errorbar(aes(ymin = lower.CL, ymax = upper.CL), width = 0.2) +
  geom_text(aes(label = .group), vjust = -1.5, size = 5) + # Add group letters (A, B, C)
  labs(title = "Machine:Employee Interaction Effect",
       x = "Machine:Employee",
       y = "Defects") +
  theme_bw()



#exercise 


#7.7
library(sasLM)
defect1

RanTest(Formula = Defects ~ Machine*Employee, 
        Data = defect1,
        Random = c("Machine", "Employee"))


library(lmerTest)
aov_random <- lmer(formula = Defects ~ (1 | Machine) + (1 | Employee) + (1 | Machine:Employee), 
                  data = defect1)
summary(aov_random)

library(GAD)
Machine_r  <- as.random(defect1$Machine)
Employee_r <- as.random(defect1$Employee)
GAD_3 <- lm(Defects ~ Machine_r * Employee_r, data = defect1)
gad(GAD_3)


#7.8
library(sasLM)
toollife
toollife_aov <- aov(formula = Lifetime ~ A + B + C, data = toollife)
summary(toollife_aov)
library(car)
Anova(mod = toollife_aov, type = 3)


toollife_aov_2 <- aov(formula = Lifetime ~ A + B + C + A:C, data = toollife)
summary(toollife_aov_2)
Anova(mod = toollife_aov_2, type = 3)


toollife_aov_3 <- aov(formula = Lifetime ~ A*B*C, data = toollife)
summary(toollife_aov_3)
Anova(mod = toollife_aov_3, type = 3)


#visualization

# 1. Calculate CLD for the full ABC interaction
# Ensure we use the ABC means to compare all combinations
ls_means_ABC <- emmeans(toollife_aov_3, ~ A * B * C)
cld_ABC_result <- multcomp::cld(ls_means_ABC, alpha = 0.05, Letters = letters, adjust = "tukey")

# 2. Convert to data frame for ggplot
cld_df <- as.data.frame(cld_ABC_result)

# 3. Create a comprehensive comparison plot
library(ggplot2)

ggplot(cld_df, aes(x = interaction(A, B, C), y = emmean, fill = B)) +
  geom_bar(stat = "identity", position = position_dodge(), alpha = 0.7) +
  geom_errorbar(aes(ymin = lower.CL, ymax = upper.CL), width = 0.2) +
  geom_text(aes(label = .group, y = upper.CL), vjust = -0.5, size = 5, fontface = "bold") +
  labs(title = "Multi-Comparison of Tool Life (A*B*C Interaction)",
       subtitle = "Groups sharing the same letter are not significantly different (Sidak adjustment)",
       x = "Combination of A.B.C",
       y = "Estimated Marginal Mean of Lifetime") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))


